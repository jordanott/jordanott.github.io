<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>A Neural Network Wrote My English Essay</title>
  <meta name="description" content="As a computer science major in the final semester of my undergrad degree, the ‘senioritis’ has started to set in. Especially when it comes to my writing GE. ...">
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://jordanott.github.io/jekyll/update/2017/05/14/neural-net-wrote-my-essay.html">
  <link rel="alternate" type="application/rss+xml" title="Jordan Ott" href="http://jordanott.github.io/feed.xml">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Jordan Ott</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">A Neural Network Wrote My English Essay</h1>
    <p class="post-meta"><time datetime="2017-05-14T00:00:00-07:00" itemprop="datePublished">May 14, 2017</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>As a computer science major in the final semester of my undergrad degree, the ‘senioritis’ has started to set in. Especially when it comes to my writing GE. Unfortunately, without this class I cannot graduate. Thus, I was presented with a very standard optimization problem. Get a high enough grade to pass, with the minimal amount of  writing required. It turns outs that we can define a function:</p>

<div><center> $$f(w) = \frac{1}{w}$$  </center></div>

<p>where <script type="math/tex">w</script> is the amount of time spent writing and the output, <script type="math/tex">f(w)</script>, is the happiness I feel. What’s great about this function is that it is continuous and therefore differentiable. Hurray optimization! It turns out that as the hours spent writing approaches zero my happiness level increases infinitely. We should definitely exploit this finding.</p>

<p>How, you ask? With a Long Short Term Memory (<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">LSTM</a>) network obviously! <strong>Spoiler alert:</strong> I’m training a neural net to write my essay for me.</p>

<p>First, we’ll need to gather lots of training data for our network. My essay is supposed to be a reflective piece on the class as a whole. To accomplish this I used the books we read for class, articles we talked about and numerous samples of my own writing.</p>

<p><a href="http://english4success.ru/Upload/books/268.pdf">The Glass Castle</a><br />
<a href="http://www.george-orwell.org/1984">1984</a><br />
<a href="http://www.cgpgrey.com/blog/humans-need-not-apply">Humans Need Not Apply</a><br />
<a href="www.jordanott.com">Jordan Ott</a><br />
<a href="http://www.seussville.com/books/book_detail.php?isbn=9780375851568">Dr. Seuss</a> is it a crime to want your model to rhyme?</p>

<p>Once we have our data we can start creating our model. We’ll be using a type of recurrent structure called an LSTM. These models are very useful when dealing with time dependent data i.e. current data samples are dependent on prior samples. Feed-forward networks are indifferent to time sensitive data and have no notion of memory. Structuring sentences depends on one’s ability to relate current words to previous one.s This is where LSTMs will be very useful.</p>

<p>I won’t go into the low level details of how they work since many others have wonderful explanations of them. We’ll use Python and Keras for our implementation which makes it very easy to stack multiple LSTM cells. Initializing our network is very straight forward.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Activation</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">RMSprop</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="c"># set return_sequences=True so we can feed the output of one cell right into another</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span><span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">maxlen</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">))))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span><span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">256</span><span class="p">))</span>
<span class="c"># add a fully connected layer for our character level predictions</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">'softmax'</span><span class="p">))</span>
<span class="c"># more optimization!</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">RMSprop</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span></code></pre></figure>

<p>If you plan to try this with a multiple layer LSTM you better have a nice GPU. I used an Nvidia TitanX and just let this run for a week or so.</p>

<h3 id="results">Results</h3>

<div class="highlighter-rouge"><pre class="highlight"><code>‘No! Not merely to extract your confession, not to punish you. Shall I
tele st te as eers aedr the the  the the th t the fall  the sode and  
and woul happe and the the ald were not and ofr the sary and we he  
I ton wasee o no the so hase t therot an I saugnt he  t he ker  
andhne the  and at was eton Whe the thi as the andile the to the  
ingou and t the the oh wge s ante we whe  the stit an and the and  
of te the the  the bast w the the the fo ande the he wonoth  
stern and animated.
</code></pre>
</div>

<p>The phrase <code class="highlighter-rouge">‘No! Not merely to extract your confession, not to punish you. Shall</code> is what is fed to the network to prompt it to generate the resulting paragraph. Everything that follows was produced by the LSTM. I think what’s very interesting about this is if you glance very quickly at it you can’t tell that it’s complete gibberish. The network learns proper spacing, punctuation and even spells a few words correctly.</p>

<p>You can find my full “essay” <a href="http://jordanott.github.io/assets/results.txt">here</a>. I hope my professor likes it!</p>

<h3 id="moving-forward">Moving Forward</h3>

<p>Character level LSTMs are nice and pretty efficient but when it comes to generating meaningful text they might not be the best choice. Think about how you write. You don’t think one letter at a time and generate a probability distribution for what letter comes next. You think in complete words with concrete ideas of what you want to convey. This area of natural language processing still has a long ways to go.</p>


  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Jordan Ott</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>Jordan Ott</li>
          <li><a href="mailto:jordanott365@gmail.com">jordanott365@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/jordanott"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">jordanott</span></a>

          </li>
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>"I have no special talent. I am only passionately curious." - Albert Einstein
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
